{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.2\n",
    "\n",
    "# 7.2.1. What are the three stages to build the hypotheses or model in machine learning?\n",
    "The three stages to build the hypotheses or model in machine learning are:\n",
    "1. Model building\n",
    "2. Model testing\n",
    "3. Applying the model\n",
    "\n",
    "#7.2.2. What is the standard approach to supervised learning?\n",
    "The standard approach to supervised learning is to split the set of example into the training data set and the testing data set.\n",
    "Supervised learning approach is suitable where we have historical leveld data. In this kind of approach we can compare between \n",
    "actual output and correct output based on that we have different methods to modify the model accordingly which best suit for \n",
    "the given dataset. The 3 differnet methods are :\n",
    "1.Classification \n",
    "2.Regression \n",
    "3.Anomaly Detection \n",
    "4.Prediction and gradient boosting\n",
    "\n",
    "# 7.2.3. What is Training set and Test set?\n",
    "Training set is a dataset based on which the model has to be developed or trained.\n",
    "Test set a dataset where the model has to be tested.\n",
    "If the model is set properly then it should be predict properly for the test data set.\n",
    "\n",
    "# 7.2.4. What is the general principle of an ensemble method and what is bagging and boosting in ensemble method?\n",
    "The general principle of an ensemble method is to combine the predictions of several models built with a given learning \n",
    "algorithm in order to improve robustness over a single model.\n",
    "Bagging :- It is a method in ensemble for improving unstable estimation or classification schemes. Bagging both can reduce \n",
    "errors by reducing the variance term.\n",
    "Boosting :- This method are used sequentially to reduce the bias of the combined model. Boosting can reduce errors by reducing \n",
    "the variance term.\n",
    "# 7.2.5. How can you avoid overfitting ?\n",
    "We can avoid overfitting by using:\n",
    "• Lots of data\n",
    "• Cross-validation (Splitting the given dataset into training dataset and testing dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
